{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9f20p7xH_Tx",
    "outputId": "0f01d14f-53b1-4f7a-dd4d-929307849d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLTCPs6PnW_E",
    "outputId": "e8bef3fd-6168-4ca0-ee83-567dcb99da28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     id                                           news_url  \\\n",
       " 0  gossipcop-2493749932  www.dailymail.co.uk/tvshowbiz/article-5874213/...   \n",
       " 1  gossipcop-4580247171  hollywoodlife.com/2018/05/05/paris-jackson-car...   \n",
       " 2   gossipcop-941805037  variety.com/2017/biz/news/tax-march-donald-tru...   \n",
       " 3  gossipcop-2547891536  www.dailymail.co.uk/femail/article-3499192/Do-...   \n",
       " 4  gossipcop-5476631226  variety.com/2018/film/news/list-2018-oscar-nom...   \n",
       " \n",
       "                                                title  \\\n",
       " 0  Did Miley Cyrus and Liam Hemsworth secretly ge...   \n",
       " 1  Paris Jackson & Cara Delevingne Enjoy Night Ou...   \n",
       " 2  Celebrities Join Tax March in Protest of Donal...   \n",
       " 3  Cindy Crawford's daughter Kaia Gerber wears a ...   \n",
       " 4      Full List of 2018 Oscar Nominations – Variety   \n",
       " \n",
       "                                            tweet_ids  \n",
       " 0  284329075902926848\\t284332744559968256\\t284335...  \n",
       " 1  992895508267130880\\t992897935418503169\\t992899...  \n",
       " 2  853359353532829696\\t853359576543920128\\t853359...  \n",
       " 3  988821905196158981\\t988824206556172288\\t988825...  \n",
       " 4  955792793632432131\\t955795063925301249\\t955798...  ,\n",
       "                  id                                           news_url  \\\n",
       " 0  gossipcop-882573  https://www.brides.com/story/teen-mom-jenelle-...   \n",
       " 1  gossipcop-875924  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
       " 2  gossipcop-894416        https://en.wikipedia.org/wiki/Quinn_Perkins   \n",
       " 3  gossipcop-857248  https://www.refinery29.com/en-us/2018/03/19192...   \n",
       " 4  gossipcop-884684  https://www.cnn.com/2017/10/04/entertainment/c...   \n",
       " \n",
       "                                                title  \\\n",
       " 0  Teen Mom Star Jenelle Evans' Wedding Dress Is ...   \n",
       " 1  Kylie Jenner refusing to discuss Tyga on Life ...   \n",
       " 2                                      Quinn Perkins   \n",
       " 3  I Tried Kim Kardashian's Butt Workout & Am For...   \n",
       " 4  Celine Dion donates concert proceeds to Vegas ...   \n",
       " \n",
       "                                            tweet_ids  \n",
       " 0  912371411146149888\\t912371528343408641\\t912372...  \n",
       " 1  901989917546426369\\t901989992074969089\\t901990...  \n",
       " 2  931263637246881792\\t931265332022579201\\t931265...  \n",
       " 3  868114761723936769\\t868122567910936576\\t868128...  \n",
       " 4  915528047004209152\\t915529285171122176\\t915530...  )"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first load the datasets to understand their structure\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the fake news dataset\n",
    "fake_news_df = pd.read_csv(\"E:\\Code\\dataset\\gossipcop_fake.csv\")\n",
    "\n",
    "# Loading the real news dataset\n",
    "real_news_df = pd.read_csv(\"E:\\Code\\dataset\\gossipcop_real.csv\")\n",
    "\n",
    "# Displaying the first few rows of each dataset to observe the data\n",
    "fake_news_df.head(), real_news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRBiV9UcnuZ-",
    "outputId": "0cf247fd-94b0-4c4a-84b9-d150f06677fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22140 entries, 0 to 22139\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         22140 non-null  object\n",
      " 1   news_url   21871 non-null  object\n",
      " 2   title      22140 non-null  object\n",
      " 3   tweet_ids  20894 non-null  object\n",
      " 4   target     22140 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 865.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                      id                                           news_url  \\\n",
       " 0  gossipcop-2493749932  www.dailymail.co.uk/tvshowbiz/article-5874213/...   \n",
       " 1  gossipcop-4580247171  hollywoodlife.com/2018/05/05/paris-jackson-car...   \n",
       " 2   gossipcop-941805037  variety.com/2017/biz/news/tax-march-donald-tru...   \n",
       " 3  gossipcop-2547891536  www.dailymail.co.uk/femail/article-3499192/Do-...   \n",
       " 4  gossipcop-5476631226  variety.com/2018/film/news/list-2018-oscar-nom...   \n",
       " \n",
       "                                                title  \\\n",
       " 0  Did Miley Cyrus and Liam Hemsworth secretly ge...   \n",
       " 1  Paris Jackson & Cara Delevingne Enjoy Night Ou...   \n",
       " 2  Celebrities Join Tax March in Protest of Donal...   \n",
       " 3  Cindy Crawford's daughter Kaia Gerber wears a ...   \n",
       " 4      Full List of 2018 Oscar Nominations – Variety   \n",
       " \n",
       "                                            tweet_ids  target  \n",
       " 0  284329075902926848\\t284332744559968256\\t284335...       0  \n",
       " 1  992895508267130880\\t992897935418503169\\t992899...       0  \n",
       " 2  853359353532829696\\t853359576543920128\\t853359...       0  \n",
       " 3  988821905196158981\\t988824206556172288\\t988825...       0  \n",
       " 4  955792793632432131\\t955795063925301249\\t955798...       0  )"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Label the datasets\n",
    "fake_news_df['target'] = 0  # Fake\n",
    "real_news_df['target'] = 1  # Real\n",
    "\n",
    "# Step 2: Combine the datasets\n",
    "combined_news_df = pd.concat([fake_news_df, real_news_df], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe structure and a sample\n",
    "combined_news_df.info(), combined_news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oKGnnJrMlbP",
    "outputId": "ad9aa290-50c9-4dd4-88b4-78e08f472149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    16817\n",
      "0     5323\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the counts of each target value in the combined dataset\n",
    "target_counts = combined_news_df['target'].value_counts()\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFC848zsN5Fh",
    "outputId": "17411045-3edc-47e7-e87f-90c57ad74df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imbalanced-learn->imblearn) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TAbZ0u0n65F",
    "outputId": "8a659aaf-bee8-4969-bf8e-cc1df5964249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (2.4.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.1-cp39-cp39-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.1-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torchvision) (1.23.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.19.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.4.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.4.1 torchvision-0.19.1\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp39-cp39-win_amd64.whl (199.3 MB)\n",
      "   ---------------------------------------- 0.0/199.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/199.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/199.3 MB 2.8 MB/s eta 0:01:12\n",
      "   ---------------------------------------- 1.8/199.3 MB 4.0 MB/s eta 0:00:50\n",
      "   ---------------------------------------- 2.4/199.3 MB 3.7 MB/s eta 0:00:53\n",
      "    --------------------------------------- 2.9/199.3 MB 3.4 MB/s eta 0:00:58\n",
      "    --------------------------------------- 3.4/199.3 MB 3.3 MB/s eta 0:01:00\n",
      "    --------------------------------------- 4.2/199.3 MB 3.1 MB/s eta 0:01:03\n",
      "    --------------------------------------- 4.7/199.3 MB 3.1 MB/s eta 0:01:03\n",
      "   - -------------------------------------- 5.2/199.3 MB 3.1 MB/s eta 0:01:04\n",
      "   - -------------------------------------- 5.8/199.3 MB 3.0 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 6.3/199.3 MB 3.0 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 6.8/199.3 MB 3.0 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 7.3/199.3 MB 2.9 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 7.9/199.3 MB 2.9 MB/s eta 0:01:06\n",
      "   - -------------------------------------- 8.7/199.3 MB 2.9 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 9.2/199.3 MB 2.9 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 9.7/199.3 MB 2.9 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 10.2/199.3 MB 2.9 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 10.7/199.3 MB 2.9 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 11.3/199.3 MB 2.8 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 12.1/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 12.6/199.3 MB 2.8 MB/s eta 0:01:07\n",
      "   -- ------------------------------------- 13.1/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 13.6/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 14.2/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   -- ------------------------------------- 14.7/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 15.2/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 16.0/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 16.5/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 17.0/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 17.6/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 18.1/199.3 MB 2.8 MB/s eta 0:01:06\n",
      "   --- ------------------------------------ 18.6/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 19.1/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   --- ------------------------------------ 19.9/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 20.4/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 21.0/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 21.5/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 22.0/199.3 MB 2.8 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 22.5/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 23.1/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 23.9/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 24.4/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ---- ----------------------------------- 24.9/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ----- ---------------------------------- 25.4/199.3 MB 2.8 MB/s eta 0:01:04\n",
      "   ----- ---------------------------------- 26.0/199.3 MB 2.8 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 26.5/199.3 MB 2.8 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 27.0/199.3 MB 2.8 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 27.5/199.3 MB 2.8 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 28.0/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 28.6/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 29.1/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ----- ---------------------------------- 29.6/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ------ --------------------------------- 30.1/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ------ --------------------------------- 30.7/199.3 MB 2.7 MB/s eta 0:01:03\n",
      "   ------ --------------------------------- 31.2/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 31.7/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 32.2/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 32.8/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 33.6/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 34.1/199.3 MB 2.7 MB/s eta 0:01:02\n",
      "   ------ --------------------------------- 34.6/199.3 MB 2.7 MB/s eta 0:01:01\n",
      "   ------- -------------------------------- 35.1/199.3 MB 2.7 MB/s eta 0:01:01\n",
      "   ------- -------------------------------- 35.7/199.3 MB 2.7 MB/s eta 0:01:01\n",
      "   ------- -------------------------------- 36.2/199.3 MB 2.7 MB/s eta 0:01:01\n",
      "   ------- -------------------------------- 37.0/199.3 MB 2.7 MB/s eta 0:01:01\n",
      "   ------- -------------------------------- 37.5/199.3 MB 2.7 MB/s eta 0:01:00\n",
      "   ------- -------------------------------- 38.0/199.3 MB 2.7 MB/s eta 0:01:00\n",
      "   ------- -------------------------------- 38.5/199.3 MB 2.7 MB/s eta 0:01:00\n",
      "   ------- -------------------------------- 39.1/199.3 MB 2.7 MB/s eta 0:01:00\n",
      "   ------- -------------------------------- 39.6/199.3 MB 2.7 MB/s eta 0:01:00\n",
      "   -------- ------------------------------- 40.1/199.3 MB 2.7 MB/s eta 0:00:59\n",
      "   -------- ------------------------------- 40.9/199.3 MB 2.7 MB/s eta 0:00:59\n",
      "   -------- ------------------------------- 41.4/199.3 MB 2.7 MB/s eta 0:00:59\n",
      "   -------- ------------------------------- 41.9/199.3 MB 2.7 MB/s eta 0:00:59\n",
      "   -------- ------------------------------- 42.5/199.3 MB 2.7 MB/s eta 0:00:58\n",
      "   -------- ------------------------------- 43.0/199.3 MB 2.7 MB/s eta 0:00:58\n",
      "   -------- ------------------------------- 43.5/199.3 MB 2.7 MB/s eta 0:00:58\n",
      "   -------- ------------------------------- 44.0/199.3 MB 2.7 MB/s eta 0:00:58\n",
      "   -------- ------------------------------- 44.6/199.3 MB 2.7 MB/s eta 0:00:58\n",
      "   --------- ------------------------------ 45.4/199.3 MB 2.7 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 45.9/199.3 MB 2.7 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 46.4/199.3 MB 2.7 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 46.9/199.3 MB 2.7 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 47.4/199.3 MB 2.7 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 48.2/199.3 MB 2.7 MB/s eta 0:00:56\n",
      "   --------- ------------------------------ 48.8/199.3 MB 2.7 MB/s eta 0:00:56\n",
      "   --------- ------------------------------ 49.3/199.3 MB 2.7 MB/s eta 0:00:56\n",
      "   --------- ------------------------------ 49.8/199.3 MB 2.7 MB/s eta 0:00:56\n",
      "   ---------- ----------------------------- 50.3/199.3 MB 2.7 MB/s eta 0:00:56\n",
      "   ---------- ----------------------------- 50.9/199.3 MB 2.7 MB/s eta 0:00:55\n",
      "   ---------- ----------------------------- 51.4/199.3 MB 2.7 MB/s eta 0:00:55\n",
      "   ---------- ----------------------------- 51.9/199.3 MB 2.7 MB/s eta 0:00:55\n",
      "   ---------- ----------------------------- 52.7/199.3 MB 2.7 MB/s eta 0:00:55\n",
      "   ---------- ----------------------------- 53.2/199.3 MB 2.7 MB/s eta 0:00:55\n",
      "   ---------- ----------------------------- 53.7/199.3 MB 2.7 MB/s eta 0:00:54\n",
      "   ---------- ----------------------------- 54.3/199.3 MB 2.7 MB/s eta 0:00:54\n",
      "   ---------- ----------------------------- 54.8/199.3 MB 2.7 MB/s eta 0:00:54\n",
      "   ----------- ---------------------------- 55.3/199.3 MB 2.7 MB/s eta 0:00:54\n",
      "   ----------- ---------------------------- 55.8/199.3 MB 2.7 MB/s eta 0:00:54\n",
      "   ----------- ---------------------------- 56.4/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 57.1/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 57.7/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 58.2/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 58.5/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 59.0/199.3 MB 2.7 MB/s eta 0:00:53\n",
      "   ----------- ---------------------------- 59.5/199.3 MB 2.7 MB/s eta 0:00:52\n",
      "   ------------ --------------------------- 60.0/199.3 MB 2.7 MB/s eta 0:00:52\n",
      "   ------------ --------------------------- 60.6/199.3 MB 2.7 MB/s eta 0:00:52\n",
      "   ------------ --------------------------- 61.1/199.3 MB 2.7 MB/s eta 0:00:52\n",
      "   ------------ --------------------------- 61.9/199.3 MB 2.7 MB/s eta 0:00:52\n",
      "   ------------ --------------------------- 62.4/199.3 MB 2.7 MB/s eta 0:00:51\n",
      "   ------------ --------------------------- 62.9/199.3 MB 2.7 MB/s eta 0:00:51\n",
      "   ------------ --------------------------- 63.4/199.3 MB 2.7 MB/s eta 0:00:51\n",
      "   ------------ --------------------------- 64.0/199.3 MB 2.7 MB/s eta 0:00:51\n",
      "   ------------ --------------------------- 64.5/199.3 MB 2.7 MB/s eta 0:00:51\n",
      "   ------------- -------------------------- 65.0/199.3 MB 2.7 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 65.8/199.3 MB 2.7 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 66.3/199.3 MB 2.7 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 66.8/199.3 MB 2.7 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 67.4/199.3 MB 2.7 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 67.9/199.3 MB 2.7 MB/s eta 0:00:49\n",
      "   ------------- -------------------------- 68.4/199.3 MB 2.7 MB/s eta 0:00:49\n",
      "   ------------- -------------------------- 68.9/199.3 MB 2.7 MB/s eta 0:00:49\n",
      "   ------------- -------------------------- 69.7/199.3 MB 2.7 MB/s eta 0:00:49\n",
      "   -------------- ------------------------- 70.3/199.3 MB 2.7 MB/s eta 0:00:49\n",
      "   -------------- ------------------------- 70.8/199.3 MB 2.7 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 71.3/199.3 MB 2.7 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 71.8/199.3 MB 2.7 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 72.4/199.3 MB 2.7 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 72.9/199.3 MB 2.7 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 73.4/199.3 MB 2.7 MB/s eta 0:00:47\n",
      "   -------------- ------------------------- 74.2/199.3 MB 2.7 MB/s eta 0:00:47\n",
      "   -------------- ------------------------- 74.7/199.3 MB 2.7 MB/s eta 0:00:47\n",
      "   --------------- ------------------------ 75.2/199.3 MB 2.7 MB/s eta 0:00:47\n",
      "   --------------- ------------------------ 75.8/199.3 MB 2.7 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 76.3/199.3 MB 2.7 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 76.8/199.3 MB 2.7 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 77.6/199.3 MB 2.7 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 78.1/199.3 MB 2.7 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 78.6/199.3 MB 2.7 MB/s eta 0:00:45\n",
      "   --------------- ------------------------ 79.2/199.3 MB 2.7 MB/s eta 0:00:45\n",
      "   --------------- ------------------------ 79.7/199.3 MB 2.7 MB/s eta 0:00:45\n",
      "   ---------------- ----------------------- 80.2/199.3 MB 2.7 MB/s eta 0:00:45\n",
      "   ---------------- ----------------------- 81.0/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 81.5/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 82.1/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 82.6/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 83.1/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 83.6/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 84.1/199.3 MB 2.7 MB/s eta 0:00:44\n",
      "   ----------------- ---------------------- 84.9/199.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ----------------- ---------------------- 85.5/199.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ----------------- ---------------------- 86.0/199.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ----------------- ---------------------- 86.5/199.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ----------------- ---------------------- 87.0/199.3 MB 2.7 MB/s eta 0:00:43\n",
      "   ----------------- ---------------------- 87.6/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 88.1/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 88.6/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 89.1/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 89.7/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ------------------ --------------------- 89.9/199.3 MB 2.7 MB/s eta 0:00:42\n",
      "   ------------------ --------------------- 90.4/199.3 MB 2.7 MB/s eta 0:00:41\n",
      "   ------------------ --------------------- 91.2/199.3 MB 2.7 MB/s eta 0:00:41\n",
      "   ------------------ --------------------- 91.8/199.3 MB 2.7 MB/s eta 0:00:41\n",
      "   ------------------ --------------------- 92.3/199.3 MB 2.7 MB/s eta 0:00:41\n",
      "   ------------------ --------------------- 92.8/199.3 MB 2.7 MB/s eta 0:00:41\n",
      "   ------------------ --------------------- 93.3/199.3 MB 2.7 MB/s eta 0:00:40\n",
      "   ------------------ --------------------- 93.8/199.3 MB 2.7 MB/s eta 0:00:40\n",
      "   ------------------ --------------------- 94.4/199.3 MB 2.7 MB/s eta 0:00:40\n",
      "   ------------------- -------------------- 94.9/199.3 MB 2.7 MB/s eta 0:00:40\n",
      "   ------------------- -------------------- 95.4/199.3 MB 2.7 MB/s eta 0:00:40\n",
      "   ------------------- -------------------- 96.2/199.3 MB 2.7 MB/s eta 0:00:39\n",
      "   ------------------- -------------------- 96.7/199.3 MB 2.7 MB/s eta 0:00:39\n",
      "   ------------------- -------------------- 97.3/199.3 MB 2.7 MB/s eta 0:00:39\n",
      "   ------------------- -------------------- 97.8/199.3 MB 2.7 MB/s eta 0:00:39\n",
      "   ------------------- -------------------- 98.3/199.3 MB 2.7 MB/s eta 0:00:39\n",
      "   ------------------- -------------------- 98.8/199.3 MB 2.7 MB/s eta 0:00:38\n",
      "   ------------------- -------------------- 99.4/199.3 MB 2.7 MB/s eta 0:00:38\n",
      "   -------------------- ------------------- 100.1/199.3 MB 2.7 MB/s eta 0:00:38\n",
      "   -------------------- ------------------- 100.7/199.3 MB 2.7 MB/s eta 0:00:38\n",
      "   -------------------- ------------------- 101.2/199.3 MB 2.7 MB/s eta 0:00:37\n",
      "   -------------------- ------------------- 101.7/199.3 MB 2.7 MB/s eta 0:00:37\n",
      "   -------------------- ------------------- 102.2/199.3 MB 2.7 MB/s eta 0:00:37\n",
      "   -------------------- ------------------- 102.8/199.3 MB 2.7 MB/s eta 0:00:37\n",
      "   -------------------- ------------------- 103.3/199.3 MB 2.7 MB/s eta 0:00:37\n",
      "   -------------------- ------------------- 103.8/199.3 MB 2.7 MB/s eta 0:00:36\n",
      "   -------------------- ------------------- 104.3/199.3 MB 2.7 MB/s eta 0:00:36\n",
      "   --------------------- ------------------ 105.1/199.3 MB 2.7 MB/s eta 0:00:36\n",
      "   --------------------- ------------------ 105.6/199.3 MB 2.7 MB/s eta 0:00:36\n",
      "   --------------------- ------------------ 106.2/199.3 MB 2.7 MB/s eta 0:00:36\n",
      "   --------------------- ------------------ 106.7/199.3 MB 2.7 MB/s eta 0:00:35\n",
      "   --------------------- ------------------ 107.2/199.3 MB 2.7 MB/s eta 0:00:35\n",
      "   --------------------- ------------------ 107.7/199.3 MB 2.7 MB/s eta 0:00:35\n",
      "   --------------------- ------------------ 108.3/199.3 MB 2.7 MB/s eta 0:00:35\n",
      "   --------------------- ------------------ 109.1/199.3 MB 2.7 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 109.6/199.3 MB 2.7 MB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 110.1/199.3 MB 2.7 MB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 110.6/199.3 MB 2.7 MB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 111.1/199.3 MB 2.7 MB/s eta 0:00:34\n",
      "   ---------------------- ----------------- 111.7/199.3 MB 2.7 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 112.2/199.3 MB 2.7 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 113.0/199.3 MB 2.7 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 113.5/199.3 MB 2.7 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 114.0/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ---------------------- ----------------- 114.6/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 115.1/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 115.6/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 116.1/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 116.7/199.3 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------- ---------------- 117.2/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 117.7/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 118.0/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 118.5/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 119.0/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ----------------------- ---------------- 119.5/199.3 MB 2.7 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 120.3/199.3 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 120.8/199.3 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 121.4/199.3 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 121.9/199.3 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------ --------------- 122.4/199.3 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 122.9/199.3 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 123.7/199.3 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------ --------------- 124.3/199.3 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 124.8/199.3 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 125.3/199.3 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 125.8/199.3 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 126.4/199.3 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 127.1/199.3 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------- -------------- 127.7/199.3 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 128.2/199.3 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 128.7/199.3 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 129.2/199.3 MB 2.7 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 129.8/199.3 MB 2.7 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 130.3/199.3 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 130.8/199.3 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 131.6/199.3 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 132.1/199.3 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 132.6/199.3 MB 2.7 MB/s eta 0:00:26\n",
      "   -------------------------- ------------- 133.2/199.3 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 133.7/199.3 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------------- ------------- 134.2/199.3 MB 2.7 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 135.0/199.3 MB 2.7 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 135.5/199.3 MB 2.7 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 136.1/199.3 MB 2.7 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 136.6/199.3 MB 2.7 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 137.1/199.3 MB 2.7 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 137.6/199.3 MB 2.7 MB/s eta 0:00:24\n",
      "   --------------------------- ------------ 138.4/199.3 MB 2.7 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 138.9/199.3 MB 2.7 MB/s eta 0:00:23\n",
      "   --------------------------- ------------ 139.5/199.3 MB 2.7 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 140.0/199.3 MB 2.7 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 140.5/199.3 MB 2.7 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 141.0/199.3 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 141.6/199.3 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 142.1/199.3 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 142.9/199.3 MB 2.7 MB/s eta 0:00:22\n",
      "   ---------------------------- ----------- 143.4/199.3 MB 2.7 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 143.9/199.3 MB 2.7 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 144.4/199.3 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 145.0/199.3 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 145.5/199.3 MB 2.7 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 146.0/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 146.5/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 147.1/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 147.6/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 148.1/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 148.6/199.3 MB 2.7 MB/s eta 0:00:20\n",
      "   ----------------------------- ---------- 149.2/199.3 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 149.7/199.3 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 150.2/199.3 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 150.7/199.3 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 151.3/199.3 MB 2.7 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 151.8/199.3 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 152.6/199.3 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 153.1/199.3 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 153.6/199.3 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------------ --------- 154.1/199.3 MB 2.7 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 154.7/199.3 MB 2.7 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 155.2/199.3 MB 2.7 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 155.7/199.3 MB 2.7 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 156.5/199.3 MB 2.7 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 157.0/199.3 MB 2.7 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 157.5/199.3 MB 2.7 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 158.1/199.3 MB 2.7 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 158.6/199.3 MB 2.7 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 159.4/199.3 MB 2.7 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 159.6/199.3 MB 2.7 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 160.4/199.3 MB 2.7 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 161.0/199.3 MB 2.7 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 161.5/199.3 MB 2.7 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 162.0/199.3 MB 2.7 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 162.5/199.3 MB 2.7 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 163.1/199.3 MB 2.7 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 163.6/199.3 MB 2.7 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 164.1/199.3 MB 2.7 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 164.9/199.3 MB 2.7 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 165.4/199.3 MB 2.7 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 165.9/199.3 MB 2.7 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 166.5/199.3 MB 2.7 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 167.0/199.3 MB 2.7 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 167.5/199.3 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 168.0/199.3 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 168.6/199.3 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 169.3/199.3 MB 2.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 169.9/199.3 MB 2.7 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 170.4/199.3 MB 2.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 170.9/199.3 MB 2.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 171.4/199.3 MB 2.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 172.0/199.3 MB 2.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 172.5/199.3 MB 2.7 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 173.3/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 173.8/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 174.1/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 174.6/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 175.1/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 175.4/199.3 MB 2.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 175.6/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 176.2/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 176.4/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 176.7/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 176.9/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 177.2/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 177.2/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 177.5/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 177.7/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 178.0/199.3 MB 2.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 178.3/199.3 MB 2.5 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 178.5/199.3 MB 2.5 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 178.8/199.3 MB 2.5 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 179.0/199.3 MB 2.5 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 179.3/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 179.8/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 180.1/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 180.4/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 180.6/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 180.9/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 181.1/199.3 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 181.4/199.3 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 181.7/199.3 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 181.9/199.3 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 182.2/199.3 MB 2.4 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 182.7/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 183.0/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 183.2/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 183.5/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 183.8/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 184.0/199.3 MB 2.4 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 184.3/199.3 MB 2.3 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 184.5/199.3 MB 2.3 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 184.8/199.3 MB 2.3 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 185.1/199.3 MB 2.3 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 185.6/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 185.9/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 186.1/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 186.4/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 186.6/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 186.9/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 187.4/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 187.7/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 188.0/199.3 MB 2.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 188.2/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 188.5/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 188.7/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 189.0/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 189.3/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 189.5/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 189.5/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 189.8/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 190.1/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 190.3/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 190.3/199.3 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 190.6/199.3 MB 2.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 190.8/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 191.1/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 191.6/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 191.9/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 192.2/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 192.4/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 192.9/199.3 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 193.2/199.3 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 193.5/199.3 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 194.0/199.3 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 194.2/199.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  194.5/199.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  194.8/199.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  195.0/199.3 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  195.6/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  195.8/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  196.1/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  196.3/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  196.6/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  196.9/199.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  197.1/199.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  197.4/199.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  197.7/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  197.9/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  198.2/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  198.4/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  198.4/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  198.7/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.0/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  199.2/199.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.3/199.3 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.6 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.6/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 1.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 1.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.0/6.2 MB 1.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.3/6.2 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.1/6.2 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.4/6.2 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.9/6.2 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.1/6.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.9/6.2 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.2/6.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.2 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.0/6.2 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/536.2 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.9.0 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.3 torch-2.4.1\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.0-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (3.10.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (2024.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (1.23.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch-geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from jinja2->torch-geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->torch-geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Downloading torch_geometric-2.6.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.5/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.8/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.0\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iifOCysdn2lL"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Remove extra spaces\n",
    "    text = re.sub(r'^b\\s+', '', text)  # Remove prefix b if exists\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the title column\n",
    "combined_news_df['cleaned_title'] = combined_news_df['title'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKKVvTPqOKKH",
    "outputId": "3d28d598-d8c0-4694-ea29-320249c1545c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: Counter({1: 13456, 0: 13456})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_features = vectorizer.fit_transform(combined_news_df['cleaned_title'])\n",
    "features = tfidf_features.toarray()\n",
    "labels = combined_news_df['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Prepare PyTorch Geometric data\n",
    "def prepare_data(X, y):\n",
    "    edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)  # Example edge index, you need to define your own based on your data\n",
    "    x_tensor = torch.tensor(X, dtype=torch.float)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor)\n",
    "    return data\n",
    "\n",
    "train_data = prepare_data(X_train_smote, y_train_smote)\n",
    "test_data = prepare_data(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ewIVeV2FsqrK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data splits\n",
    "labels = torch.tensor(combined_news_df['target'].values, dtype=torch.long)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    tfidf_features.toarray(), labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_features, train_labels, test_size=0.25, random_state=42  # 0.25 x 0.8 = 0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aulCwRC_EmDm",
    "outputId": "deccc8c4-29e6-4468-969d-8ed2be6fc7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hkQV-5WMEFSf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define model\n",
    "model = GNN(input_dim=1000, hidden_dim=64, output_dim=2)  # input_dim matches TF-IDF features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3xgdZte7LZHt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaik\\.conda\\envs\\tf_gpu\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Convert numpy arrays to tensors\n",
    "train_features_tensor = torch.tensor(train_features, dtype=torch.float)\n",
    "val_features_tensor = torch.tensor(val_features, dtype=torch.float)\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float)\n",
    "\n",
    "def create_edge_index(tfidf_features, threshold=0.3):\n",
    "    cosine_sim = cosine_similarity(tfidf_features)\n",
    "    edges = []\n",
    "    num_nodes = cosine_sim.shape[0]\n",
    "    for idx1 in range(num_nodes):\n",
    "        for idx2 in range(idx1 + 1, num_nodes):\n",
    "            if cosine_sim[idx1, idx2] > threshold:\n",
    "                edges.append((idx1, idx2))\n",
    "                edges.append((idx2, idx1))  # Ensure undirected graph\n",
    "    return torch.tensor(edges, dtype=torch.long).t()\n",
    "\n",
    "edge_index_train = create_edge_index(X_train_smote)\n",
    "edge_index_test = create_edge_index(X_test)\n",
    "\n",
    "# Prepare PyTorch Geometric data\n",
    "train_data = Data(x=torch.tensor(X_train_smote, dtype=torch.float), edge_index=edge_index_train, y=torch.tensor(y_train_smote, dtype=torch.long))\n",
    "test_data = Data(x=torch.tensor(X_test, dtype=torch.float), edge_index=edge_index_test, y=torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def plot_graph(edge_index, max_edges=100):\n",
    "    print(\"Plotting...\")\n",
    "    G = nx.Graph()\n",
    "    edge_index = edge_index.t().cpu().numpy()\n",
    "    edges = [(int(x), int(y)) for x, y in edge_index[:max_edges]]  # Limit the number of edges plotted\n",
    "    G.add_edges_from(edges)\n",
    "    pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=70)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges, width=0.5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "E0eJEmR6LpbG",
    "outputId": "ad59445a-c169-4320-aeb1-0928a4743bf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Training Progress:   0%|                                                                | 0/50 [00:00<?, ?it/s]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.57s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:   2%|▎                | 1/50 [00:01<01:18,  1.60s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:   4%|▋                | 2/50 [00:03<01:14,  1.56s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:   6%|█                | 3/50 [00:04<01:12,  1.55s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:   8%|█▎               | 4/50 [00:06<01:11,  1.55s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  10%|█▋               | 5/50 [00:07<01:09,  1.54s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.56s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  12%|██               | 6/50 [00:09<01:08,  1.56s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  14%|██▍              | 7/50 [00:10<01:07,  1.57s/it, test_accuracy=0.789, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  16%|██▋              | 8/50 [00:12<01:05,  1.56s/it, test_accuracy=0.789, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  18%|███              | 9/50 [00:14<01:03,  1.56s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.50s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  20%|███▏            | 10/50 [00:15<01:01,  1.55s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  22%|███▌            | 11/50 [00:17<01:00,  1.55s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  24%|███▊            | 12/50 [00:18<00:58,  1.55s/it, test_accuracy=0.788, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  26%|████▏           | 13/50 [00:20<00:57,  1.56s/it, test_accuracy=0.788, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.50s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  28%|████▍           | 14/50 [00:21<00:55,  1.55s/it, test_accuracy=0.788, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  30%|████▊           | 15/50 [00:23<00:54,  1.56s/it, test_accuracy=0.787, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.49s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  32%|█████           | 16/50 [00:24<00:52,  1.55s/it, test_accuracy=0.787, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  34%|█████▍          | 17/50 [00:26<00:51,  1.56s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.488]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it, loss=0.488]\u001b[A\n",
      "Overall Training Progress:  36%|█████▊          | 18/50 [00:28<00:49,  1.56s/it, test_accuracy=0.787, train_loss=0.488]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.60s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  38%|██████          | 19/50 [00:29<00:48,  1.58s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  40%|██████▍         | 20/50 [00:31<00:47,  1.59s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  42%|██████▋         | 21/50 [00:32<00:46,  1.59s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  44%|███████         | 22/50 [00:34<00:44,  1.60s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.57s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  46%|███████▎        | 23/50 [00:36<00:43,  1.60s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  48%|███████▋        | 24/50 [00:37<00:41,  1.58s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.58s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  50%|████████        | 25/50 [00:39<00:39,  1.59s/it, test_accuracy=0.786, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  52%|████████▎       | 26/50 [00:40<00:37,  1.56s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  54%|████████▋       | 27/50 [00:42<00:35,  1.56s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  56%|████████▉       | 28/50 [00:43<00:34,  1.57s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.61s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  58%|█████████▎      | 29/50 [00:45<00:33,  1.59s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.56s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  60%|█████████▌      | 30/50 [00:47<00:31,  1.59s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  62%|█████████▉      | 31/50 [00:48<00:29,  1.58s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.52s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  64%|██████████▏     | 32/50 [00:50<00:28,  1.57s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.486]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.50s/it, loss=0.486]\u001b[A\n",
      "Overall Training Progress:  66%|██████████▌     | 33/50 [00:51<00:26,  1.55s/it, test_accuracy=0.787, train_loss=0.486]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch Progress:   0%|                                                       | 0/1 [00:01<?, ?it/s, loss=0.487]\u001b[A\n",
      "Training Epoch Progress: 100%|███████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/it, loss=0.487]\u001b[A\n",
      "Overall Training Progress:  68%|██████████▉     | 34/50 [00:53<00:24,  1.56s/it, test_accuracy=0.787, train_loss=0.487]\n",
      "Training Epoch Progress:   0%|                                                                   | 0/1 [00:01<?, ?it/s]\u001b[A\n",
      "Overall Training Progress:  68%|██████████▉     | 34/50 [00:54<00:25,  1.61s/it, test_accuracy=0.787, train_loss=0.487]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Training Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m epochs:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[1;32m---> 43\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m         test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(test_loader)\n\u001b[0;32m     45\u001b[0m         epochs\u001b[38;5;241m.\u001b[39mset_postfix(train_loss\u001b[38;5;241m=\u001b[39mtrain_loss, test_accuracy\u001b[38;5;241m=\u001b[39mtest_accuracy)\n",
      "Cell \u001b[1;32mIn[28], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf_gpu\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm  # Use auto to better handle different environments like Jupyter Notebooks.\n",
    "\n",
    "# Define the model and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with managed progress bar\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Epoch Progress\", leave=True)  # leave=True ensures the bar stays after completion\n",
    "    for data in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())  # Update progress bar with current loss\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.max(1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Run training and evaluation with a single progress bar for all epochs\n",
    "with tqdm(range(num_epochs), desc=\"Overall Training Progress\", leave=True) as epochs:\n",
    "    for epoch in epochs:\n",
    "        train_loss = train()\n",
    "        test_accuracy = evaluate(test_loader)\n",
    "        epochs.set_postfix(train_loss=train_loss, test_accuracy=test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (conv1): GCNConv(1000, 64)\n",
       "  (conv2): GCNConv(64, 2)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the model safely\n",
    "model = GNN(input_dim=1000, hidden_dim=64, output_dim=2)\n",
    "model.load_state_dict(torch.load('model_GNN.pth', weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample News Features: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.5311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4814, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4916, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4945, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Actual Label (0-Fake, 1-Real): 0\n",
      "Predicted Label (0-Fake, 1-Real): 0\n"
     ]
    }
   ],
   "source": [
    "# Function to predict and display a sample news item\n",
    "\n",
    "def predict_sample(test_loader):\n",
    "    for data in test_loader:  # Normally you would loop over the whole dataset, but here we'll just take the first batch\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.max(1)[1]  # Get the index of the max log-probability\n",
    "        break  # We only take one batch for demonstration\n",
    "    \n",
    "    sample_idx = 90 # Index of the sample to display\n",
    "    sample_news = data.x[sample_idx]  # The features of the sample news\n",
    "    actual_label = data.y[sample_idx].item()  # Actual label\n",
    "    predicted_label = pred[sample_idx].item()  # Predicted label\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Sample News Features:\", sample_news)\n",
    "    print(\"Actual Label (0-Fake, 1-Real):\", actual_label)\n",
    "    print(\"Predicted Label (0-Fake, 1-Real):\", predicted_label)\n",
    "\n",
    "# Call the function\n",
    "predict_sample(test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "if_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
